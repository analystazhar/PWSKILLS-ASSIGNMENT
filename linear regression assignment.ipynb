{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789118ce",
   "metadata": {},
   "source": [
    "# Q1. ANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d6282",
   "metadata": {},
   "source": [
    "Simple Linear Regression and Multiple Linear Regression are both statistical techniques used in machine learning\n",
    "and statistics to model the relationship between one or more independent variables (features) and a dependent\n",
    "variable (target). Here's a breakdown of the key differences between the two, along with examples for each:\n",
    "\n",
    "(1) Simple Linear Regression:\n",
    "Simple Linear Regression involves only one independent variable (feature).\n",
    "It models the relationship between this single feature and the target variable.\n",
    "Example:\n",
    "Let's say you want to predict a student's final exam score (y) based on the number of hours they spent studying (x).\n",
    "You collect data on several students' study hours and exam scores. Using simple linear regression,\n",
    "you can find the relationship between study hours and exam scores. Here's an example dataset:\n",
    "    Study Hours (x)     Exam Score (y)\n",
    "    2                        60\n",
    "    3                        70\n",
    "    4                        75\n",
    "    5                        85\n",
    "    6                        90\n",
    "\n",
    "(2) Multiple Linear Regression:\n",
    "Multiple Linear Regression involves two or more independent variables (features).\n",
    "It models the relationship between these multiple features and the target variable.\n",
    "\n",
    "Example:\n",
    "Let's say you want to predict a house's price (y) based on various factors such as square footage (x1),\n",
    "number of bedrooms (x2), and distance to the nearest school (x3). You collect data on several houses and \n",
    "their corresponding prices and features. Using multiple linear regression, you can find the relationship\n",
    "between these multiple features and house prices.\n",
    "The dataset might look like this:\n",
    "    Square Footage(x1)     Bedrooms(x2)  Distance to School(x3)      House Price (y)\n",
    "    1500                        3                0.5                       250,000\n",
    "    2000                        4                1.0                       320,000\n",
    "    1200                        2                0.8                       200,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371fb8fb",
   "metadata": {},
   "source": [
    "# Q2. ANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4011a74",
   "metadata": {},
   "source": [
    "Linear regression is a powerful statistical method for modeling the relationship between a dependent\n",
    "variable (target) and one or more independent variables (features). However, to make valid inferences\n",
    "and predictions using linear regression, several assumptions must hold. It's essential to check these\n",
    "assumptions to ensure that the linear regression model is appropriate for your data. Here are the key\n",
    "assumptions of linear regression:\n",
    "\n",
    "(1)Linearity: The relationship between the independent variables (features) and the dependent variable\n",
    "    (target) should be linear. This means that the change in the target variable is proportional to changes\n",
    "    in the independent variables. You can check this assumption by visualizing the data using scatter plots\n",
    "    or residual plots and ensuring that the points approximately form a straight line.\n",
    "\n",
    "(2)Independence of Errors: The errors (residuals) should be independent of each other. In other words, the\n",
    "    value of the error for one data point should not depend on the values of the errors for other data points.\n",
    "    To check this assumption, you can use residual plots and look for patterns or correlations among residuals.\n",
    "\n",
    "(3)Homoscedasticity (Constant Variance of Errors): The variance of the errors should be constant across all levels\n",
    "    of the independent variables. This means that the spread of residuals should be roughly consistent as you move\n",
    "    along the range of the independent variables. You can check for homoscedasticity by plotting residuals against\n",
    "    the predicted values or the independent variables and looking for patterns.\n",
    "\n",
    "(4)Normality of Errors: The errors (residuals) should follow a normal distribution. Linear regression assumes that\n",
    "    the residuals are normally distributed with a mean of zero. You can check this assumption by creating a histogram\n",
    "    or a Q-Q plot of the residuals and looking for a roughly normal distribution.\n",
    "\n",
    "(5)No or Little Multicollinearity: In multiple linear regression (with more than one independent variable), the independent \n",
    "    variables should not be highly correlated with each other. High multicollinearity can make it challenging to separate the\n",
    "    individual effects of each independent variable on the target variable. You can check for multicollinearity using\n",
    "    correlation matrices or variance inflation factors (VIF).\n",
    "    \n",
    "    \n",
    "   ## Here are some common methods to check the assumptions of linear regression:\n",
    "\n",
    "    (1).Visual Inspection: Create scatter plots of the independent variables against the dependent variable and \n",
    "        residual plots to visually assess linearity, homoscedasticity, and normality of errors.\n",
    "\n",
    "   (2)Normality Tests: Use statistical tests like the Shapiro-Wilk test or Anderson-Darling test to check for \n",
    "    normality of residuals. Additionally, Q-Q plots can help visualize the distribution of residuals compared \n",
    "    to a normal distribution.\n",
    "\n",
    "    (3)Residual Plots: Plot residuals against predicted values or independent variables to detect patterns that \n",
    "        violate assumptions.\n",
    "\n",
    "   (4) Variance Inflation Factor (VIF): Calculate the VIF for each independent variable in multiple linear\n",
    "    regression to assess multicollinearity. VIF values greater than 1 indicate potential multicollinearity.\n",
    "\n",
    "    (5)Durbin-Watson Test: This test checks for autocorrelation (dependence of errors) in time-series data. \n",
    "        A value close to 2 indicates no autocorrelation.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf700a39",
   "metadata": {},
   "source": [
    "# Q3. ANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b55615",
   "metadata": {},
   "source": [
    "We will understand about slope and intercept in a linear regression with the real world data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdffdeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce239f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('placement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba0bee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cgpa</th>\n",
       "      <th>placement_exam_marks</th>\n",
       "      <th>placed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.19</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.46</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.54</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.23</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>8.87</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9.12</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>4.89</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8.62</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4.90</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cgpa  placement_exam_marks  placed\n",
       "0    7.19                  26.0       1\n",
       "1    7.46                  38.0       1\n",
       "2    7.54                  40.0       1\n",
       "3    6.42                   8.0       1\n",
       "4    7.23                  17.0       0\n",
       "..    ...                   ...     ...\n",
       "995  8.87                  44.0       1\n",
       "996  9.12                  65.0       1\n",
       "997  4.89                  34.0       0\n",
       "998  8.62                  46.0       1\n",
       "999  4.90                  10.0       1\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3e03448",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.iloc[:,0].values.reshape(-1,1)\n",
    "y=df.iloc[:,-1].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "454957ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "612e35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078dc103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we are training with the whole data set for now\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7b4adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bfc806e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0220969]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the slope..\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1543aefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33517816])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the intercept....\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a989e24",
   "metadata": {},
   "source": [
    "# Q4. ANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be0af4",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used in machine learning and numerical optimization\n",
    "to minimize a cost or loss function by iteratively adjusting the parameters of a model. It's a \n",
    "fundamental technique for training machine learning models, particularly those that involve finding the \n",
    "optimal parameters for a given problem. Gradient Descent is widely used in various machine learning algorithms,\n",
    "including linear regression, logistic regression, neural networks, and more.\n",
    "\n",
    "In summary, Gradient Descent is a crucial technique in machine learning that helps optimize models by minimizing\n",
    "a cost or loss function. It iteratively adjusts model parameters to find the values that result in the best model\n",
    "performance on the training data. It's a fundamental concept for training and fine-tuning machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbe7c5",
   "metadata": {},
   "source": [
    "# Q5 ANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaacefa",
   "metadata": {},
   "source": [
    "Multiple Linear Regression is an extension of simple linear regression, a statistical technique used in\n",
    "machine learning and statistics for modeling the relationship between a dependent variable (target) and\n",
    "multiple independent variables (features). While simple linear regression deals with a single independent\n",
    "variable, multiple linear regression handles two or more independent variables.\n",
    "\n",
    "In summary, the primary difference between simple linear regression and multiple linear regression is the \n",
    "number of independent variables involved. Multiple linear regression allows you to consider the combined \n",
    "effects of multiple features on the target variable and is used when there are multiple factors influencing \n",
    "the outcome of interest. It provides a more comprehensive understanding of how a set of features collectively \n",
    "affects the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc538a",
   "metadata": {},
   "source": [
    "# Q6 ANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2744ce4",
   "metadata": {},
   "source": [
    "Multicollinearity is a common issue in multiple linear regression, and it occurs when two or more\n",
    "independent variables in a regression model are highly correlated with each other. In other words, \n",
    "multicollinearity means that some of the predictor variables are not independent of each other,\n",
    "making it challenging to determine their individual effects on the dependent variable. Multicollinearity \n",
    "can lead to unstable parameter estimates, reduced model interpretability, and less reliable predictions. \n",
    "Here's a more detailed explanation and how to detect and address multicollinearity:\n",
    "Causes of Multicollinearity:\n",
    "Multicollinearity can arise from various sources, including:\n",
    "\n",
    "(1)Data collection methods: If two or more variables are collected using similar methods or instruments, \n",
    "they are more likely to be highly correlated.\n",
    "(2)Overlapping or redundant variables: When two variables measure very similar aspects of the same phenomenon, \n",
    "they tend to be correlated.\n",
    "(3)Transformation of variables: Creating new variables through transformations (e.g., squaring, taking logarithms)\n",
    " of existing variables can introduce multicollinearity.\n",
    "(4)Interaction terms: Including interaction terms in a regression model can also lead to multicollinearity, \n",
    "especially when the main effects involved in the interaction are included.\n",
    "\n",
    "##Detecting Multicollinearity:\n",
    "There are several methods to detect multicollinearity:\n",
    "(1)Correlation Matrix: Calculate the correlation coefficients between pairs of independent variables.\n",
    "    High correlation coefficients (close to 1 or -1) indicate multicollinearity.\n",
    "(2)Variance Inflation Factor (VIF): Calculate the VIF for each independent variable.\n",
    "    VIF measures how much the variance of the estimated regression coefficients is increased due to multicollinearity.\n",
    "    High VIF values (typically greater than 5 or 10) suggest multicollinearity.\n",
    "(3)Eigenvalues and Condition Indices: Analyze the eigenvalues of the correlation matrix or the condition indices of the \n",
    "design matrix. Large eigenvalues or condition indices indicate multicollinearity.\n",
    "(4)Scatterplots and Regression Diagnostics: Visualize the relationships between independent variables using \n",
    " scatterplots and check for patterns that suggest multicollinearity. Additionally, examine regression diagnostic \n",
    "plots for unusual behavior.\n",
    "\n",
    "\n",
    "##Addressing Multicollinearity:\n",
    "Once multicollinearity is detected, you can take several steps to address the issue:\n",
    "\n",
    "(1)Remove Redundant Variables: If two or more variables are highly correlated and provide similar information, \n",
    "consider removing one of them from the model.\n",
    "(2)Combine Variables: Create composite variables by averaging or summing correlated variables to reduce multicollinearity.\n",
    "(3)Feature Selection: Use feature selection techniques to identify and keep only the most important variables in the model.\n",
    "(4)Principal Component Analysis (PCA): Apply PCA to transform the original variables into a new set of uncorrelated variables\n",
    "(principal components) while preserving most of the variance.\n",
    "(5)Regularization: Use regularization techniques like Ridge or Lasso regression, which introduce penalties to the model \n",
    "coefficients, helping to mitigate multicollinearity.\n",
    "(6)Collect More Data: Sometimes, multicollinearity is due to a small sample size. Collecting more data may help reduce the \n",
    "issue.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926afb07",
   "metadata": {},
   "source": [
    "# Q7. ANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b752647",
   "metadata": {},
   "source": [
    "Polynomial regression is a variation of linear regression, a statistical technique used in machine learning and \n",
    "statistics to model the relationship between a dependent variable (target) and one or more independent variables\n",
    "(features). While linear regression assumes a linear relationship between the independent variables and the target,\n",
    "polynomial regression allows for more complex, nonlinear relationships to be captured.\n",
    "\n",
    "Polynomial Regression Model:\n",
    "\n",
    "In polynomial regression, the relationship between the independent variable (xx) and the target variable (yy) is modeled \n",
    "as a polynomial function of the form:\n",
    "\n",
    "y=β0+β1⋅x+β2⋅x2+β3⋅x3+…+βn⋅xn+εy=β0​+β1​⋅x+β2​⋅x2+β3​⋅x3+…+βn​⋅xn+ε\n",
    "\n",
    "   ..yy represents the dependent variable (the target you're trying to predict).\n",
    "    ..xx represents the independent variable.\n",
    "    ..β0β0​ is the intercept (also known as the constant or bias term).\n",
    "    ..β1,β2,…,βnβ1​,β2​,…,βn​ are the coefficients associated with each power of xx (e.g., linear, quadratic, cubic, etc.).\n",
    "    ..εε represents the error term, which accounts for the variability in the dependent variable that the model cannot explain.\n",
    "\n",
    "In summary, polynomial regression extends the capabilities of linear regression by allowing for nonlinear relationships \n",
    "between the independent and dependent variables. While linear regression is suitable for modeling linear trends, polynomial \n",
    "regression can better capture complex, nonlinear patterns in the data. However, one must be cautious about overfitting when \n",
    "using polynomial regression with high-degree terms. The choice between linear and polynomial regression depends on the nature\n",
    "of the data and the underlying relationship between the variables.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d63b77",
   "metadata": {},
   "source": [
    "# Q8. ANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed9899",
   "metadata": {},
   "source": [
    "Polynomial regression offers both advantages and disadvantages compared to linear regression, and the choice between\n",
    "the two depends on the nature of the data and the underlying relationships between variables. Here are the advantages\n",
    "and disadvantages of polynomial regression relative to linear regression:\n",
    "\n",
    "##Advantages of Polynomial Regression:\n",
    "\n",
    "(1)Captures Nonlinear Patterns: The primary advantage of polynomial regression is its ability to capture and model nonlinear \n",
    "relationships between the independent and dependent variables. Linear regression is limited to modeling linear\n",
    "relationships, while polynomial regression can handle curved, bent, or oscillating patterns.\n",
    "\n",
    "(2)Improved Fit: When the data exhibits nonlinear behavior, polynomial regression can provide a better fit to the data than\n",
    "linear regression. This can lead to more accurate predictions and improved model performance.\n",
    "\n",
    "(3)Flexibility: Polynomial regression is flexible and can adapt to various data distributions and patterns. By adjusting the\n",
    "degree of the polynomial (e.g., linear, quadratic, cubic, etc.), you can fine-tune the model to match the complexity of \n",
    "the data.\n",
    "\n",
    "##Disadvantages of Polynomial Regression:\n",
    "\n",
    "(1)Overfitting: One of the significant disadvantages of polynomial regression is its susceptibility to overfitting. \n",
    "When using high-degree polynomials, the model can fit the training data extremely closely, but it may not generalize \n",
    "well to new, unseen data. Regularization techniques may be required to address overfitting.\n",
    "\n",
    "(2)Complexity: Polynomial regression models with high-degree terms can become mathematically complex and challenging to\n",
    "interpret. Interpretability decreases as the degree of the polynomial increases.\n",
    "\n",
    "(3)Data Requirements: Polynomial regression may require a relatively large amount of data to accurately estimate the model\n",
    "parameters, especially when using high-degree polynomials. Insufficient data can lead to unstable parameter estimates.\n",
    "\n",
    "##When to Prefer Polynomial Regression:\n",
    "\n",
    "Polynomial regression is a useful tool when dealing with specific situations:\n",
    "\n",
    "(1)Nonlinear Data: When there is clear evidence of nonlinear relationships between the independent and dependent variables,\n",
    "polynomial regression is a good choice. For example, when plotting the data reveals a curved or bent pattern, polynomial \n",
    "regression can better capture this behavior.\n",
    "\n",
    "(2)Complex Patterns: When the relationship between variables is intricate and linear regression is too simplistic to model \n",
    "it accurately, polynomial regression allows you to capture complex patterns and variations in the data.\n",
    "\n",
    "(3)Domain Knowledge: If you have domain knowledge or theoretical reasons to believe that a polynomial relationship exists,\n",
    "using polynomial regression can help align the model with prior expectations.\n",
    "\n",
    "(4)Exploratory Data Analysis: Polynomial regression can be a valuable tool in exploratory data analysis (EDA) when you want\n",
    "to understand the underlying data patterns before choosing the final model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706be656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
